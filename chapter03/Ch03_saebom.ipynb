{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 지도학습\n",
        "\n",
        "* 지도 학습에서는 좋은 모델을 만들기 위해 학습 과정에서 데이터를 이용한 예측값과 실제값을 맞춰 보는 과정이 핵심 구조라고 할 수 있다.\n",
        "* 지도 학습은 데이터에 대응하는 라벨(label) 혹은 목푯값(target value)이라고 불리는 실제 정답을 가지고 있어야 하기 때문에 실제값이 없을 경우에는 모델 구축 이전에 실제값을 정의해야만 한다.\n",
        "* 우리가 데이터를 가지고 지도 학습을 하기 위해서는 어떤 사진이 어떤 동물인지 표시를 하는 라벨링 작업(labelling 혹은 annotation)을 하거나 폴더 정리를 거쳐야만 한다.\n",
        "* 지도 학습의 목표는 학습을 통해 적절한 모델의 가중치와 편향을 찾는 것이다."
      ],
      "metadata": {
        "id": "jLyFnf6GzY10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실 함수 (Loss Function)\n",
        "* 실제값과 예측값이 얼마나 차이가 나는지를 측정하는 척도로써 문제에 따라 적절한 손실 함수를 정해주는 것이 중요하다.\n",
        "* 현재 예측을 통해 얻은 손실 함숫값 보다 다음 학습 시 더 작은 손실 함숫값을 얻기 위해 이전 가중치와 편향을 좀 더 적절한 가중치와 편향으로 최적화(Optimization)하게 된다.\n"
      ],
      "metadata": {
        "id": "fVE_klyi0tdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 역전파(Back-propagation)\n",
        "* 미분 계산으로 구성된 역전파를 통해 손실 함수의 최소 지점을 찾게 된다.\n",
        "* 이 과정에서 새로운 가중치와 편향으로 다시 예측값을 구해서 손실 함수를 계산하고 역전파를 통해 다시 가중치와 편향을 업데이트하는 일련의 과정을 반복하게 된다.\n"
      ],
      "metadata": {
        "id": "-NkcF60N1ezd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ...\n",
        "train_loader = ...\n",
        "# 반복 학습\n",
        "for epoch in range(num_epochs): # 전체 데이터에 대한 반복 학습\n",
        "  for inputs, labels in train_loader: # 입력값과 실제값 - 배치 데이터\n",
        "    optimizer.zero_grad() # 최적화 초기화\n",
        "    outputs  = model(inputs) # 예측값 산출\n",
        "    loss = criterion(outputs, labels) # 손실 함수 계산\n",
        "    loss.backward() # 손실 함수 기준으로 역전파 설정\n",
        "    optimizer.step() # 모델 가중치 업데이트"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "nVhC8B_C2Ujs",
        "outputId": "27eb2947-d00d-41eb-ccc6-36a7a8bfec97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-845a7fdb25b5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 반복 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 전체 데이터에 대한 반복 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 입력값과 실제값 - 배치 데이터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 최적화 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 배치(Batch) 데이터\n",
        "* 많은 양의 데이터를 한 번에 계산하면 메모리 문제가 발생할 수 있다.\n",
        "* 따라서 데이터를 분할해서 학습에 사용한다.\n",
        "* 예를 들어 50000개의 학습 데이터가 있다면 20개씩 나눠 2500번의 내부 for문을 돌게 된다."
      ],
      "metadata": {
        "id": "WdoxBxJy1zn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회귀 문제(Regression)\n",
        "* 우리가 원하는 결과값이 연속적인 변수인 것을 예측하는 문제이다.\n",
        "* 집값 예측, 온도 예측, 대기질 예측\n"
      ],
      "metadata": {
        "id": "ZbDyW64p1zwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 분류 문제(Classification)\n",
        "* 우리가 원하는 결과값이 클래스(Class)라고 하는 유한한모임으로 분류되는 문제다.\n",
        "* 질병 예측(양성(1) 또는 음성(0)), 만족도 예측(1, 2, 3점)이 있다.\n",
        "* 만족도를 1, 2, 3이라고 구분 짓는 것을 라벨링(Labelling)이라고 한다.\n",
        "* 만약 1, 2, 3을 0과 1로만 구성된 벡터 (1, 0, 0), (0, 1, 0), (0, 0, 1)로 표현하는 방법을 원-핫 인코딩(One-hot encoding)이라고 부른다."
      ],
      "metadata": {
        "id": "FdO2S4y-1z3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 세트 분할\n",
        "* 평가 데이터로부터 얻은 결과를 기준으로 가장 좋은 모델을 선택한다면 평가 데이터 외 새로운 데이터에 대해서 예측을 잘못할 수 있기 때문에 학습, 검증(Validation), 평가용으로 데이터를 3 종류로 나누기도 한다.\n",
        "* 학습 : 검증: 평가 = 6 : 2 : 2로 일반적으로 나눈다.\n",
        "* 가장 중요한 것은 어떠한 방식으로 데이터를 학습, 검증, 평가 세트로 나누더라도 데이터가 절대 중복으로 들어가서는 안된다."
      ],
      "metadata": {
        "id": "sirEENxH3Yuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = np.arange(1000).reshape((100, 10)), np.arange(100)\n",
        "# test_size = 0.3인 경우에는 평가 데이터가 전체의 30%이므로 100개 중 무작위로 골라\n",
        "# 70개가 학습 데이터가 되고 나머지 30개가 평가 데이터가 된다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "metadata": {
        "id": "qPNVSwIV4meE"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}