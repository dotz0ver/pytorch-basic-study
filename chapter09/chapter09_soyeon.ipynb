{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPgR3K1N6TimtOvJWfnaADR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Chapter 9 성능개선"],"metadata":{"id":"rMWWzVv4O8L5"}},{"cell_type":"markdown","source":["## 9.1 과적합\n","- 학습 데이터에 대해서는 예측을 잘 하지만 테스트 데이터와 같은 학습에 사용되지 않은 데이터에 대해서는 예측을 잘 못하는 현상\n","- 이 과적합을 방지하면서 딥런닝 모델을 학습시키는 것이 정규화 방법"],"metadata":{"id":"6L_RSq0zO-hp"}},{"cell_type":"markdown","source":["### 9.1.1 데이터 증식\n","- 딥러닝은 많은 데이터를 요구하지만 추가 데이터를 확보하는 건 쉽지 않음. 그래서 Data augmentation 기법을 통해 임의로 새로운 데이터를 만들어 학습 데이터에 추가"],"metadata":{"id":"SIUhL4kuPT-q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDctFK3cnQwF"},"outputs":[],"source":["import torchvision.transforms as tr\n","import PIL\n","\n","# 60x60으로 이미지 일부를 무작위로 잘라서 같은 이미지라도 매번 다른 입력 이미지로\n","transf = tr.Compose([tr.ToPILImage(), tr.RandomCrop(60),\n","tr.ColorJitter(brightness=0.1,contrast=0.1,saturation=0.1,hue=0.1), # 이미지 밝기, 대비, 색조 변형\n","                     tr.RandomHorizontalFlip(),tr.ToTensor()]) # 이미지 뒤집기"]},{"cell_type":"markdown","source":["### 9.1.2 조기 종료\n","- 모델이 학습 데이터를 많이 공부한다면 학습 데이터에 맞춰져 모델이 최적화될 수 있으므로 적당한 기준을 정하여 모델 학습을 끊는 것이 Early Stopping 방법\n","- 손실 함수값이 작다고 반드시 정확도가 높은 것은 아님\n","- 학습 반복 횟수를 더 크게 할 경우 시험 데이터의 손실 함수값이 다시 내려오는 경우도 존재\n","- 시험 데이터는 오직 평가에만 사용. 모델 선택 시에는 검증 데이터 사용할 것"],"metadata":{"id":"cO2KF8bDSzK7"}},{"cell_type":"code","source":["# 라이브러리 불러오기\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt"],"metadata":{"id":"9nclB-yET0uT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습, 검증, 평가 데이터 생성하기\n","\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]\n",")\n","dataset = torchvision.datasets.CIFAR10(root='./data',train=True,\n","                                       download=True,transform=transform)\n","# dataset 분할\n","trainset, valset = torch.utils.data.random_split(dataset,[30000,20000])\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=32, shuffle=False)\n","testset = torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_sizze=32, shuffle=False)"],"metadata":{"id":"RCnM36MfUNhJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.3 L2 정규화\n","- 경계를 만들어 학습 데이터에서의 최적 변수 w(**)*에 도달하지 못하게 하며 경계 안에서만 변수를 최적화하도록 함. 따라서 v(*)로 수렴하게 됨"],"metadata":{"id":"Rbw4BfD4WHWh"}},{"cell_type":"code","source":["optimizer = optim.Adam(resnet.parameters(), lr=1e-3, weight_decay=1e-3)"],"metadata":{"id":"ughZxkwNWxLU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.4 드롭아웃\n","- 인공 신경망에서 무작위로 일정한 비율의 노드를 제외하여 학습하는 방법 (과적합 방지)\n","- 출력층은 예측값이 나오는 단계이므로 적용하지 않음"],"metadata":{"id":"Wn654f9eWwDT"}},{"cell_type":"code","source":["class Regressor(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.fc1 = nn.Linear(13,50)\n","    self.fc2 = nn.Linear(50,1)\n","    # 해당 노드에 50%를 선택해 노드를 사용하지 않겠다는 의미\n","    self.dropout = nn.Dropout(0.5)\n","\n","  def forward(self, x):\n","    x = self.dropout(F.relu(self.fc1(x)))\n","    x = F.relu(self.fc2(x))\n","    return x"],"metadata":{"id":"3r0msXAMXY-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.5 배치 정규화\n","- 미니 배치를 이용하면 학습을 반복할 때마다 우리가 나눠 놓은 미니 배치들이 돌아가면서 사용되는데, 이때 학습을 한 번 할 때마다 입력값의 분포가 다르고 각 레이어의 입력값 분포 또한 다름\n","- 이전 층의 노드가 관련 변수들과 일차결합 연산을 거치고 그 값이 활성화 함수를 통해 다음 레이어로 가는 흐름에서, 활성화 함수로 들어가기 전 각 노드로 들어오는 값인 feature 값을 보정된 Normalization을 통해 항상 동일한 분포 위에 있게 함\n","- 입력값들의 분포를 일정하게 하여 일반적으로 학습에 대한 수렴 속도가 빠름 (overfitting 방지)"],"metadata":{"id":"OTKldbHPYA6U"}},{"cell_type":"code","source":["nn.BatchNorm2d() # 층과 층 사이"],"metadata":{"id":"k0Q3OnTJ9FWB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.6 교란 라벨 (DisturbLabel)\n","- 분류 문제에서 일정 비율만큼 라벨을 의도적으로 잘못된 라벨로 만들어서 학습을 방해하는 방법. 과적합을 막을 수 있음"],"metadata":{"id":"jbGm3He29IIF"}},{"cell_type":"markdown","source":["예를 들어 클래스 수가 10개이고 교란 라벨 비율이 30% 라면 self.p_e=73/100, self.p_i=3/100이 되고, 실제 라벨이 5라면 뽑힐 라벨 분포는 (3/100, 3/100, 3/100, 3/100, 3/100, 73/100, 3/100, 3/100, 3/100, 3/100) 이 된다.\n","\n","=> self.p_i는 나머지 30%를 교란된 라벨 수인 9개로 나누어 주면 됨. 즉, 3.33%\n","총 합은 1이 되도록 normalize"],"metadata":{"id":"VDoZ_HEhIQu0"}},{"cell_type":"code","source":["# 교란 라벨 정의하기\n","\n","class DisturbLabel(torch.nn.Module):\n","  # 교란 라벨 비율(0~100), 클래스 수\n","  def __init__(self,alpha,num_classes):\n","    super(DisturbLabel,self).__init__()\n","    self.alpha = alpha\n","    self.C = num_classes\n","    # 실제 라벨을 뽑을 확률(p_c)와 나머지(p_i)\n","    self.p_c = (1-((self.C - 1) / self.C) * (alpha / 100))\n","    self.p_i = (1-self.p_c)/(self.C-1)"],"metadata":{"id":"O7afeCocGOsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def forward(self,y):\n","  # 라벨이 뽑힐 확률 분포를 만들어줌 (3/100, 3/100...73/100...)\n","  y_tensor = y.type(torch.LongTensor).view(-1,1)\n","  depth = self.C\n","  y_one_hot = torch.ones(y_tensor.size()[0], depth) * self.p_i\n","  y_one_hot.scatter_(1,y_tensor, self.p_c)\n","  # 해당 확률을 이용해 Multinoulli 분포를 통해 샘플 뽑기\n","  y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,)))\n","  distribution = torch.distributions.OneHotCategorical(y_one_hot)\n","  y_disturbed = distribution.sample()\n","  # 10개의 원소 중 가장 큰 값의 라벨을 뽑음\n","  # 교란 라벨 비율이 30%라 해서 반드시 미니 배치의 30%가 교란 라벨인 건 아님\n","  y_disturbed = y_disturbed.max(dim=1)[1]\n","  return y_disturbed"],"metadata":{"id":"-5yGZ2rcIVtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 교란 라벨 선언 및 적용하기\n","\n","disturblabels = DisturbLabel(alpha=30, num_classes=10)"],"metadata":{"id":"tIQUSJQkZRl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(50):\n","  running_loss = 0.0\n","  for data in trainloader:\n","    inputs, labels = data[0].to(device), data[1].to(device)\n","    optimizer.zero_grad()\n","    outputs = resnet(inputs)\n","    labels = disturblabels(labels).to(device)\n","    loss = criterion(outputs, labels)\n","    ..."],"metadata":{"id":"bD_PyZn0ZgqT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.7 교란 값 DisturbValue\n","- 회귀 문제에서 일정 비율만큼 라벨에 노이즈를 주입하여 학습 데이터에 대해 최적화를 방해하는 방법"],"metadata":{"id":"rFKG-aEfZ4jo"}},{"cell_type":"code","source":["# 노이즈 생성하기\n","\n","def noise_generator(x,alpha): # 타깃값, 노이즈 비율(0~1)\n","  noise = torch.normal(0,1e-8,size=(len(x),1)) # 임의로 정한 정규분포에 따른 노이즈 생성\n","  noise[torch.randint(0,len(x),(int(len(x)*(1-alpha)),))] = 0 # 노이즈 타깃이 아닌 값은 노이즈를 0으로\n","  return noise"],"metadata":{"id":"K2IV21GraGzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 교란 값 선언 및 적용하기\n","\n","for epoch in range(400):\n","  for data in trainloader:\n","    inputs, values = data\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    values = values + noise_generator(values,alpha)\n","    loss = criterion(outputs,values)\n","    ..."],"metadata":{"id":"M6CXMPpEao1X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1.8 라벨 스무딩 Label Smoothing\n","- 분류 문제에서 사용하는 원-핫 벡터는 0과 1로 구성되어 있고 우리는 소프트맥스나 시그모이드 함수를 통해 0과 1 사이의 예측값을 출력\n","- 이때, 실제값을 0과 1이 아닌, 예측값에 가깝게 낮추거나 높인 어느 정도만 맞춰도 모델이 정답이라 판단하여 과적합을 막아주는 개념\n","- 크로스 엔트로피 함수 nn.CrossEntropyLoss()는 실제 라벨의 원 핫 벡터를 입력으로 받을 수 없으므로 사용할 수 있도록 별도로 손실 함수를 만들어 줘야 함"],"metadata":{"id":"TAR2Ul0WvbnO"}},{"cell_type":"code","source":["# 라벨 스무딩 정의 및 선언하기\n","\n","class LabelSmoothingLoss(nn.Module):\n","  def __init__(self,classes,smoothing=0.0,dim=-1):\n","    super(LabelSmoothingLoss,self).__init__()\n","    self.confidence = 1.0 - smoothing\n","    self.smoothing = smoothing\n","    self.cls = classes\n","    self.dim = dim\n","\n","  def forward(self,pred,target):\n","    # Cross Entropy 부분의 log softmax를 미리 계산\n","    pred = pred.log_softmax(dim=self.dim)\n","    with torch.no_grad():\n","      # 예측값과 동일한 크기의 영텐서를 만듦\n","      true_dist = torch.zeros_like(pred)\n","      # alpha / (K-1)\n","      true_dist.fill_(self.smoothing / (self.cls - 1))\n","      # (1-alpha)y + alpha/(K-1)\n","      true_dist.scatter_(1,target.data.unsqueeze(1),self.confidence)\n","    # pred를 함께 사용해 Cross Entropy Loss 함수 계산\n","    return torch.mean(torch.sum(-true_dist*pred,dim=self.dim))"],"metadata":{"id":"Hh38WyBIxXN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cifar10 데이터를 이용하는 경우, 클래스는 10개로 지정하고 적절한 스무딩 비율을 넣어\n","# nn.CrossEntropyLoss() 대신 LabelSmoothingLoss로 criterion 선언\n","criterion = LabelSmoothingLoss(classes=10,smoothing=0.2)"],"metadata":{"id":"ripmGhR6y9uM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9.2 데이터 불균형 Data imbalance\n","- 과적합 야기"],"metadata":{"id":"T2tvVtDVzsbr"}},{"cell_type":"markdown","source":["## 9.2.1 가중 무작위 샘플링 Weighted random sampling\n","- 불균형 데이터라도 미니 배치를 균형 데이터(각 클래스를 동일한 개수)로 뽑는다면 한 번 학습 시 균형 데이터를 사용하게 되는 것을 말함"],"metadata":{"id":"cTfSyMGsz5V-"}},{"cell_type":"code","source":["# 가중치 함수 만들기\n","import numpy as np\n","\n","def make_weights(labels,nclasses): # 라벨, 클래스 수\n","  labels = np.array(labels)\n","  weight_list = []\n","  # 각 클래스마다 라벨의 개수를 셈\n","  for cls in range(nclasses):\n","    idx = np.where(labels == cls)[0]\n","    count = len(idx)\n","    # 라벨이 뽑힐 가중치를 1/count로 동일하게 해당 라벨 전체에 할당\n","    weight = 1/count\n","    weights = [weight] * count\n","    # 데이터를 불러올 때 ImageFolder을 사용할 경우 라벨이 0부터 N까지 차례로 나열이 되어 있기에 각 클래스 가중치를 일렬로\n","    weight_list += weights\n","  return weight_list"],"metadata":{"id":"Ha-qvy6Tz1dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지 데이터 불러오기\n","# 데이터는 클래스가 2개인 이미지 데이터 (각각 14,4개)\n","\n","transf = tr.Compose([tr.Resize((16,16)),tr.ToTensor()])\n","trainset = torchvision.datasets.ImageFolder(root='./class',transform=transf)"],"metadata":{"id":"SoIjL6Nl2tPO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가중치 생성하기 후 텐서로 변환\n","# 결과: 모든 데이터에 대한 각각의 가중치가 있으며, 각 클래스의 가중치의 합이 1로 같음 (하나의 클래스를 뽑을 확률이 같다는 의미)\n","\n","weights = make_weights(trainset.targets,len(trainset.classes))\n","weights = torch.DoubleTensor(weights)\n","print(weights)"],"metadata":{"id":"UIAjYueG2uOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터로더 생성하기\n","\n","# WeightedRandomSampler를 사용하여 배치를 불러올 때 자동으로 클래스에 대한 균일 분포를 갖는 배치를 만들 수 있음\n","sampler = torch.utils.data.sampler.WeightedRandomSampler(weights,len(weights))\n","trainloader_wrs = DataLoader(trainset,batch_size=6,sampler=sampler) # sampler 추가하여 데이터 준비 완료\n","trainloader_rs = DataLoader(trainset,batch_size=6,shuffle=True) # 가중 무작위 샘플링과 무작위 샘플링 비교를 위해 무작위 샘플링을 하는 DataLoader 추가"],"metadata":{"id":"zNcJSTc03LDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가중 무작위 샘플링 vs 무작위 샘플링\n","# 배치 사이즈 6개일 경우 2개의 클래스가 각각 3개로 들어오는 것이 이상적일 것...\n","\n","for epoch in range(5):\n","  for data in trainloader_wrs:\n","    print(data[1])"],"metadata":{"id":"z-LT8ucg31Ft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(5):\n","  for data in trainloader_rs:\n","    print(data[1])"],"metadata":{"id":"r2FTnmT04FUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.2 가중 손실 함수\n","- nn.CrossEntropyLoss는 가중 손실 함수 제공"],"metadata":{"id":"KZQm3gb37HnY"}},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# 가중 손실 함수는 데이터가 적은 클래스에 대해서 큰 가중치를 부여함으로써 업데이트 균형을 맞춤\n","num_ins = [40,45,30,62,70,153,395,46,75,194]\n","# 1 - 각 클래스의 확률\n","weights = [1-(x/sum(num_ins)) for x in num_ins]\n","class_weights = torch.FloatTensor(weights).to(device)\n","criterion = nn.CrossEntropyLoss(weight=class_weights)"],"metadata":{"id":"DODCcf727NBQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2.3 혼동 행렬 Confusion matrix\n","- 데이터 불균형의 직접적인 해결책은 될 수 없지만 결과를 행렬화하여 각 클래스의 분포와 정확도를 확인하여 불균형 데이터로 예측 쏠림 현상을 인지할 수 있음"],"metadata":{"id":"qWiYcH9L__mg"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","\n","actual = [1,1,1,0,0,0,0,0,2,2,2,2,2,2,2,2]\n","prediction = [1,2,2,0,2,2,1,2,0,1,0,2,2,2,2,2]\n","# 혼동 행렬 틀을 만들어 플로팅\n","c_mat = confusion_matrix(actual,prediction)\n","plt.figure(figsize=(8,6))\n","sns.heatmap(c_mat,annot=True,fmt=\"d\",cmap='Blues',linewidths=.5)\n","# 위 아래 범위 0.5씩 더 확보\n","b,t = plt.ylim()\n","b += 0.5\n","t -= 0.5\n","plt.ylim(b,t)\n","plt.savefig('confusion_matrix.png')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"5xxHH7i_Bkw0","executionInfo":{"status":"ok","timestamp":1706780394532,"user_tz":-540,"elapsed":1411,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"6d92b933-0278-4e2c-d662-8ee0b0a956d3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## 9.3 전이 학습 Transfer learning\n","- 기존의 지식을 우리가 풀고자 하는 문제에 적용하여 학습하는 방법"],"metadata":{"id":"Lle8tcdBCkTi"}},{"cell_type":"markdown","source":["### 9.3.1 사전 학습 모델 pretrained model"],"metadata":{"id":"cGstWvSwCtxJ"}},{"cell_type":"code","source":["# CIFAR10을 위한 ResNet18 불러오기\n","# 불러온 모델은 CIFAR10보다 큰 ImageNet 데이터에 맞춰진 모델이므로\n","# 원래 첫 번째 합성곱 층의 필터 사이즈가 7x7인데 CIFAR10 데이터에는 큰 사이즈므로 3x3 필터로 변경함\n","\n","model = torchvision.models.resnet18(pretrained=True)\n","model.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1)\n","# 원래 모델의 마지막 출력 노드가 1000이기 때문에 우리 데이터에 맞춰 10개로 변경\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs,10)\n","model = model.to(device)"],"metadata":{"id":"RvRlWXGFHXI5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.3.2 모델 프리징 Model freezing\n","- 기존의 일부 모델 변수들을 그대로 사용하기 위해 업데이트가 되지 않도록 하는 방법"],"metadata":{"id":"WpiVHT3NHXxr"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torch.nn as nn\n","\n","# device 설정\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"Ttquxz1RH_52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 불러오기\n","\n","model = torchvision.models.alexnet(pretrained=True)\n","num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs,10)\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGnoG06NHhkR","executionInfo":{"status":"ok","timestamp":1706781846698,"user_tz":-540,"elapsed":6732,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"e05aa11d-6b68-4c05-c4c9-59acfa10e681"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n","100%|██████████| 233M/233M [00:05<00:00, 47.6MB/s]\n"]}]},{"cell_type":"code","source":["# 모델 파라미터명 확인하기\n","\n","# 가중치, 편향 목록 (0~9번째는 합성곱 층에 대한 가중치와 편향, 10번 이후는 분류기의 가중치와 편향)\n","for i, (name, param) in enumerate(model.named_parameters()):\n","  print(i,name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNvLey_YHxPq","executionInfo":{"status":"ok","timestamp":1706781848809,"user_tz":-540,"elapsed":325,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"b4bb9209-e75a-4dcc-c729-babc06d3e8f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 features.0.weight\n","1 features.0.bias\n","2 features.3.weight\n","3 features.3.bias\n","4 features.6.weight\n","5 features.6.bias\n","6 features.8.weight\n","7 features.8.bias\n","8 features.10.weight\n","9 features.10.bias\n","10 classifier.1.weight\n","11 classifier.1.bias\n","12 classifier.4.weight\n","13 classifier.4.bias\n","14 classifier.6.weight\n","15 classifier.6.bias\n"]}]},{"cell_type":"code","source":["# 변수 프리징하기\n","\n","for i, (name, param) in enumerate(model.named_parameters()):\n","  param.requires_grad = False # False로 두어 학습시 업데이트가 되지 않도록\n","  if i == 9: # 합성곱 층에 대한 가중치와 편향(9번까지)만 프리징이 되면 for문 멈춤\n","    print('end')\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFLkDEpQJnYs","executionInfo":{"status":"ok","timestamp":1706782305374,"user_tz":-540,"elapsed":340,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"79c68e4b-7e4b-4f74-d32c-04a599ce5ad0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["end\n"]}]},{"cell_type":"code","source":["# Requires_grad 확인하기\n","\n","f_list = [0,3,6,8,10]\n","c_list = [1,4,6]\n","for i in f_list:\n","  print(model.features[i].weight.requires_grad)\n","  print(model.features[i].bias.requires_grad)\n","for j in c_list:\n","  print(model.classifier[j].weight.requires_grad)\n","  print(model.classifier[j].bias.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KwnkvE5SKA9v","executionInfo":{"status":"ok","timestamp":1706782462668,"user_tz":-540,"elapsed":402,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"d41fe688-d17c-4f31-caf3-87b9f4eb9953"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","False\n","True\n","True\n","True\n","True\n","True\n","True\n"]}]},{"cell_type":"markdown","source":["## 9.4 준지도 학습 Semi-supervised learning\n","- 정답이 있는 데이터와 정답이 없는 데이터를 함께 사용하여 모델을 학습시키는 방법"],"metadata":{"id":"CgOuCpDBKbFG"}},{"cell_type":"markdown","source":["### 9.4.1 의사 라벨링 Pseudo labeling\n","- 준지도 학습 중 가장 기본적으로 사용되는 방법으로, 우리가 라벨이 없는 데이터를 지도 학습에 사용하려면 라벨을 달아 주어야 함\n","- 따라서 이미 학습된 모델을 이용하여 라벨링이 되지 않는 데이터를 예측한 후, 그 예측값을 기준으로 라벨링을 하여 기존의 학습 데이터와 함께 학습에 사용하는 방법이 의사 라벨링"],"metadata":{"id":"th4K2doaKqVD"}},{"cell_type":"code","source":["# 라이브러리 불러오기\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm # for문의 진행 상황을 알려주는 라이브러리"],"metadata":{"id":"0i9Hc1izNE75","executionInfo":{"status":"ok","timestamp":1706791927026,"user_tz":-540,"elapsed":539,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# GPU 연산 정의\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GxIKwXDoNeN5","executionInfo":{"status":"ok","timestamp":1706791928671,"user_tz":-540,"elapsed":4,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"5a253f38-7ff9-4dc2-8618-b0729d895c49"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","source":["# 데이터셋 정의하기\n","\n","class MyDataset(Dataset):\n","  def __init__(self,x_data,y_data,transform=None):\n","    self.x_data = x_data\n","    self.y_data = y_data\n","    self.transform = transform\n","    self.len = len(y_data)\n","\n","  def __getitem__(self,index):\n","    sample = self.x_data[index], self.y_data[index]\n","    if self.transform:\n","      sample = self.transform(sample)\n","    return sample\n","\n","  def __len__(self):\n","    return self.len"],"metadata":{"id":"yQG32GtzNsqH","executionInfo":{"status":"ok","timestamp":1706791930871,"user_tz":-540,"elapsed":2,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# 데이터 전처리 정의하기\n","\n","class TrainTransform:\n","  def __call__(self,sample):\n","    inputs, labels = sample\n","    transf = transforms.Compose([transforms.ToPILImage(),\n","                                 transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n","    final_output = transf(inputs)\n","    return final_output, labels"],"metadata":{"id":"8n5tt32KOXpy","executionInfo":{"status":"ok","timestamp":1706792322838,"user_tz":-540,"elapsed":612,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# 데이터, 라벨, 클래스 수, 나눠지는 목표 데이터 개수\n","def balanced_subset(data, labels, num_cls, num_data):\n","    num_data_per_class = num_data // num_cls  # 각 클래스의 데이터 개수 정의\n","\n","    # 나눠지는 두 개의 세트를 저장하기 위해 데이터와 라벨 텐서를 각각 2개씩 정의\n","    data1 = torch.empty((0,) + data.shape[1:], dtype=data.dtype)\n","    data2 = torch.empty((0,) + data.shape[1:], dtype=data.dtype)\n","    labels1 = torch.empty((0,), dtype=labels.dtype)\n","    labels2 = torch.empty((0,), dtype=labels.dtype)\n","\n","    # 각 클래스마다 정의된 데이터 개수만큼 무작위로 뽑아 저장\n","    for cls in range(num_cls):\n","        idx = torch.nonzero(labels == cls, as_tuple=True)[0]\n","        shuffled_idx = torch.randperm(len(idx))\n","        data1 = torch.cat([data1, data[idx[shuffled_idx[:num_data_per_class]]]], dim=0)\n","        data2 = torch.cat([data2, data[idx[shuffled_idx[num_data_per_class:]]]], dim=0)\n","        labels1 = torch.cat([labels1, labels[idx[shuffled_idx[:num_data_per_class]]]], dim=0)\n","        labels2 = torch.cat([labels2, labels[idx[shuffled_idx[num_data_per_class:]]]], dim=0)\n","\n","    return data1, data2, labels1, labels2\n"],"metadata":{"id":"QTheRaHWO0P4","executionInfo":{"status":"ok","timestamp":1706792323371,"user_tz":-540,"elapsed":2,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["# 숫자 판별 MNIST 데이터 불러오기\n","# 라벨링된 데이터가 2000개만 있다고 가정하고 학습 데이터, 검증 데이터를 각각 1000개씩 사용하며 나머지 데이터는 라벨링이 안 된 데이터라 가정\n","\n","trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n","labeled_data, unlabeled_data, labels, unlabels = balanced_subset(trainset.data, trainset.targets, num_cls=10, num_data=2000)\n","train_images, val_images, train_labels, val_labels = balanced_subset(labeled_data, labels, num_cls=10, num_data=1000)"],"metadata":{"id":"ieAwko81Rvtu","executionInfo":{"status":"ok","timestamp":1706792325065,"user_tz":-540,"elapsed":609,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["# 데이터로더 정의하기\n","# CNN은 4차원 입력 이미지가 필요함. 하지만 1채널인 MNIST 데이터는 (배치사이즈, 이미지 너비, 이미지 높이) 형태인 3차원이기 때문에\n","# unsqueeze(1)를 통해 3차원 텐서를 4차원 텐서 (배치사이즈, 1, 이미지 너비, 이미지 높이)로 변환함\n","\n","train_images = train_images.unsqueeze(1)\n","val_images = val_images.unsqueeze(1)\n","# 학습 데이터에는 Traintransform() 적용, 나머지 세트 5, 8에 대해서는 적용X\n","trainset = MyDataset(train_images, train_labels, transform=TrainTransform())\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n","validationset = MyDataset(val_images, val_labels)\n","valloader = torch.utils.data.DataLoader(validationset, batch_size=128, shuffle=False)\n","unlabeled_images = unlabeled_data.unsqueeze(1)\n","unlabeledset = MyDataset(unlabeled_images, unlabels)\n","unlabeledloader = torch.utils.data.DataLoader(unlabeledset, batch_size=256, shuffle=True)"],"metadata":{"id":"fS35BZCdcPuo","executionInfo":{"status":"ok","timestamp":1706792325065,"user_tz":-540,"elapsed":2,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["# 평가 데이터 불러옴\n","transform = transforms.Compose([transforms.ToTensor()])\n","testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset,batch_size=100,shuffle=False)"],"metadata":{"id":"O54oevRghfhC","executionInfo":{"status":"ok","timestamp":1706792325065,"user_tz":-540,"elapsed":2,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# 모델 정의하기\n","\n","class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.features = nn.Sequential(\n","        nn.Conv2d(1,64,3), nn.ReLU(),\n","        nn.MaxPool2d(2,2),\n","        nn.Conv2d(64,192,3,padding=1), nn.ReLU(),\n","        nn.MaxPool2d(2,2)\n","    )\n","    self.classifier = nn.Sequential(\n","        nn.Dropout(0.3),\n","        nn.Linear(192*6*6,1024), nn.ReLU(),\n","        nn.Dropout(0.5),\n","        nn.Linear(1024,512), nn.ReLU(),\n","        nn.Linear(512,10)\n","    )\n","\n","  def forward(self,x):\n","    x = self.features(x)\n","    x = x.view(-1,192*6*6)\n","    x = self.classifier(x)\n","    return x\n","\n","model = Net().to(device)"],"metadata":{"id":"kABY2IUXh3jk","executionInfo":{"status":"ok","timestamp":1706792325684,"user_tz":-540,"elapsed":2,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# 손실 함수 및 최적화 기법 정의하기\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"S5frCVw_k0Ab","executionInfo":{"status":"ok","timestamp":1706792325685,"user_tz":-540,"elapsed":2,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["# 정확도 평가 함수 정의하기\n","\n","def accuracy(dataloader):\n","  correct = 0\n","  total = 0\n","  with  torch.no_grad():\n","    model.eval()\n","    for data in dataloader:\n","      images, labels = data[0].float().to(device), data[1].to(device)\n","      outputs = model(images)\n","      _,predicted = torch.max(outputs.data,1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","  acc = 100*correct/total\n","  model.train()\n","  return acc"],"metadata":{"id":"3RlyOzaAlDCJ","executionInfo":{"status":"ok","timestamp":1706792704142,"user_tz":-540,"elapsed":3,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["# 지도 학습 수행하기\n","# 기준 성능을 알아보기 위해 1000개의 라벨링된 데이터로만 학습 진행\n","\n","best_acc = 0 # 검증 정확도 기준으로 모델 저장하기 위해 변수 선언\n","# 학습 데이터를 이용해 학습을 하고 학습 정확도 계산\n","for epoch in range(501):\n","  correct = 0\n","  total = 0\n","  for traindata in trainloader:\n","    inputs, labels = traindata[0].to(device).float(), traindata[1].to(device)\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    _,predicted = torch.max(outputs.detach(),1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","    # 검증 정확도를 계산하여 가장 높은 검증 정확도를 기준으로 모델 파라미터를 저장\n","    val_acc = accuracy(valloader)\n","    if val_acc >= best_acc:\n","      best_acc = val_acc\n","      torch.save(model.state_dict(), './models/cifar_model_for_pseudo_baseline.pth')\n","      print('[%d] train acc: %.2f, validation acc: %.2f - Saved the best model' %(epoch,100*correct/total,val_acc))\n","    elif epoch % 10 == 0: # epoch 10마다 학습 정확도와 검증 정확도 출력\n","      print('[%d] train acc: %.2f, validation acc: %.2f' %(epoch,100*correct/total,val_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RENVRCAJmiAL","executionInfo":{"status":"ok","timestamp":1706792991495,"user_tz":-540,"elapsed":266442,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"7c2e2b2e-0ade-4a13-e5c4-69412b87ea2c"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["[0] train acc: 17.97, validation acc: 20.30 - Saved the best model\n","[0] train acc: 17.97, validation acc: 37.20 - Saved the best model\n","[0] train acc: 22.40, validation acc: 29.00\n","[0] train acc: 24.02, validation acc: 34.10\n","[0] train acc: 26.56, validation acc: 44.50 - Saved the best model\n","[0] train acc: 28.91, validation acc: 62.90 - Saved the best model\n","[0] train acc: 32.03, validation acc: 49.90\n","[0] train acc: 32.80, validation acc: 46.40\n","[1] train acc: 56.90, validation acc: 63.30 - Saved the best model\n","[1] train acc: 57.25, validation acc: 66.70 - Saved the best model\n","[1] train acc: 57.50, validation acc: 67.00 - Saved the best model\n","[2] train acc: 65.10, validation acc: 67.00 - Saved the best model\n","[2] train acc: 69.20, validation acc: 70.40 - Saved the best model\n","[3] train acc: 77.34, validation acc: 71.40 - Saved the best model\n","[3] train acc: 78.78, validation acc: 74.10 - Saved the best model\n","[3] train acc: 78.46, validation acc: 75.90 - Saved the best model\n","[3] train acc: 78.50, validation acc: 77.00 - Saved the best model\n","[4] train acc: 75.78, validation acc: 77.00 - Saved the best model\n","[4] train acc: 80.86, validation acc: 77.20 - Saved the best model\n","[4] train acc: 81.25, validation acc: 77.50 - Saved the best model\n","[5] train acc: 84.77, validation acc: 78.60 - Saved the best model\n","[5] train acc: 83.59, validation acc: 80.20 - Saved the best model\n","[5] train acc: 84.38, validation acc: 81.40 - Saved the best model\n","[5] train acc: 84.69, validation acc: 81.70 - Saved the best model\n","[6] train acc: 87.50, validation acc: 82.60 - Saved the best model\n","[7] train acc: 86.33, validation acc: 83.40 - Saved the best model\n","[7] train acc: 86.72, validation acc: 84.60 - Saved the best model\n","[7] train acc: 86.33, validation acc: 85.20 - Saved the best model\n","[7] train acc: 86.72, validation acc: 85.20 - Saved the best model\n","[10] train acc: 91.41, validation acc: 82.30\n","[10] train acc: 92.58, validation acc: 82.80\n","[10] train acc: 91.67, validation acc: 82.90\n","[10] train acc: 91.02, validation acc: 83.30\n","[10] train acc: 91.09, validation acc: 86.20 - Saved the best model\n","[10] train acc: 91.54, validation acc: 86.40 - Saved the best model\n","[10] train acc: 91.41, validation acc: 86.70 - Saved the best model\n","[10] train acc: 91.90, validation acc: 86.60\n","[11] train acc: 91.41, validation acc: 87.30 - Saved the best model\n","[12] train acc: 91.21, validation acc: 87.60 - Saved the best model\n","[13] train acc: 94.53, validation acc: 87.70 - Saved the best model\n","[14] train acc: 95.09, validation acc: 87.80 - Saved the best model\n","[14] train acc: 95.00, validation acc: 88.50 - Saved the best model\n","[15] train acc: 96.88, validation acc: 88.60 - Saved the best model\n","[18] train acc: 96.65, validation acc: 88.90 - Saved the best model\n","[18] train acc: 96.80, validation acc: 90.80 - Saved the best model\n","[19] train acc: 99.22, validation acc: 91.50 - Saved the best model\n","[20] train acc: 93.75, validation acc: 85.40\n","[20] train acc: 94.92, validation acc: 88.30\n","[20] train acc: 96.35, validation acc: 90.20\n","[20] train acc: 96.09, validation acc: 90.00\n","[20] train acc: 95.94, validation acc: 90.80\n","[20] train acc: 95.96, validation acc: 90.80\n","[20] train acc: 95.98, validation acc: 89.90\n","[20] train acc: 95.90, validation acc: 89.00\n","[26] train acc: 97.99, validation acc: 91.70 - Saved the best model\n","[26] train acc: 98.00, validation acc: 91.70 - Saved the best model\n","[27] train acc: 96.88, validation acc: 91.80 - Saved the best model\n","[30] train acc: 98.44, validation acc: 89.30\n","[30] train acc: 98.44, validation acc: 89.60\n","[30] train acc: 98.70, validation acc: 89.80\n","[30] train acc: 98.83, validation acc: 89.40\n","[30] train acc: 98.75, validation acc: 88.90\n","[30] train acc: 98.70, validation acc: 89.20\n","[30] train acc: 98.77, validation acc: 89.30\n","[30] train acc: 98.80, validation acc: 90.20\n","[34] train acc: 99.09, validation acc: 91.90 - Saved the best model\n","[34] train acc: 99.11, validation acc: 92.20 - Saved the best model\n","[39] train acc: 99.22, validation acc: 92.30 - Saved the best model\n","[40] train acc: 100.00, validation acc: 89.40\n","[40] train acc: 99.61, validation acc: 88.80\n","[40] train acc: 98.70, validation acc: 90.00\n","[40] train acc: 98.83, validation acc: 90.70\n","[40] train acc: 98.59, validation acc: 91.10\n","[40] train acc: 98.57, validation acc: 91.40\n","[40] train acc: 98.77, validation acc: 92.00\n","[40] train acc: 98.90, validation acc: 92.30 - Saved the best model\n","[41] train acc: 99.22, validation acc: 92.70 - Saved the best model\n","[50] train acc: 98.44, validation acc: 89.40\n","[50] train acc: 98.83, validation acc: 90.00\n","[50] train acc: 99.22, validation acc: 90.10\n","[50] train acc: 99.41, validation acc: 90.80\n","[50] train acc: 99.53, validation acc: 91.00\n","[50] train acc: 99.48, validation acc: 90.80\n","[50] train acc: 99.33, validation acc: 91.00\n","[50] train acc: 99.10, validation acc: 91.10\n","[60] train acc: 100.00, validation acc: 89.60\n","[60] train acc: 99.22, validation acc: 89.30\n","[60] train acc: 98.96, validation acc: 90.30\n","[60] train acc: 98.83, validation acc: 90.90\n","[60] train acc: 99.06, validation acc: 91.10\n","[60] train acc: 99.09, validation acc: 90.90\n","[60] train acc: 99.00, validation acc: 90.80\n","[60] train acc: 99.00, validation acc: 90.40\n","[70] train acc: 99.22, validation acc: 90.20\n","[70] train acc: 99.22, validation acc: 90.40\n","[70] train acc: 99.48, validation acc: 90.40\n","[70] train acc: 99.61, validation acc: 90.50\n","[70] train acc: 99.38, validation acc: 90.80\n","[70] train acc: 99.48, validation acc: 91.50\n","[70] train acc: 99.33, validation acc: 91.00\n","[70] train acc: 99.40, validation acc: 91.30\n","[80] train acc: 99.22, validation acc: 90.60\n","[80] train acc: 99.61, validation acc: 90.20\n","[80] train acc: 99.74, validation acc: 90.30\n","[80] train acc: 99.80, validation acc: 90.70\n","[80] train acc: 99.84, validation acc: 90.90\n","[80] train acc: 99.87, validation acc: 90.90\n","[80] train acc: 99.89, validation acc: 90.70\n","[80] train acc: 99.80, validation acc: 90.90\n","[90] train acc: 97.66, validation acc: 90.60\n","[90] train acc: 97.66, validation acc: 90.80\n","[90] train acc: 98.44, validation acc: 90.50\n","[90] train acc: 98.63, validation acc: 90.10\n","[90] train acc: 98.91, validation acc: 89.60\n","[90] train acc: 98.96, validation acc: 88.20\n","[90] train acc: 98.88, validation acc: 86.80\n","[90] train acc: 99.00, validation acc: 85.60\n","[100] train acc: 100.00, validation acc: 87.30\n","[100] train acc: 100.00, validation acc: 87.50\n","[100] train acc: 100.00, validation acc: 87.40\n","[100] train acc: 100.00, validation acc: 87.50\n","[100] train acc: 99.69, validation acc: 88.30\n","[100] train acc: 99.61, validation acc: 88.80\n","[100] train acc: 99.55, validation acc: 89.30\n","[100] train acc: 99.50, validation acc: 90.00\n","[110] train acc: 100.00, validation acc: 89.10\n","[110] train acc: 100.00, validation acc: 89.30\n","[110] train acc: 100.00, validation acc: 89.40\n","[110] train acc: 99.80, validation acc: 89.90\n","[110] train acc: 99.84, validation acc: 90.00\n","[110] train acc: 99.87, validation acc: 89.70\n","[110] train acc: 99.67, validation acc: 90.70\n","[110] train acc: 99.70, validation acc: 90.70\n","[120] train acc: 100.00, validation acc: 88.60\n","[120] train acc: 100.00, validation acc: 88.60\n","[120] train acc: 100.00, validation acc: 88.50\n","[120] train acc: 99.80, validation acc: 87.90\n","[120] train acc: 99.38, validation acc: 87.40\n","[120] train acc: 99.48, validation acc: 86.30\n","[120] train acc: 99.55, validation acc: 85.60\n","[120] train acc: 99.60, validation acc: 85.20\n","[130] train acc: 99.22, validation acc: 90.00\n","[130] train acc: 99.22, validation acc: 90.60\n","[130] train acc: 99.48, validation acc: 90.60\n","[130] train acc: 99.61, validation acc: 90.60\n","[130] train acc: 99.69, validation acc: 90.40\n","[130] train acc: 99.74, validation acc: 90.10\n","[130] train acc: 99.67, validation acc: 90.80\n","[130] train acc: 99.70, validation acc: 91.70\n","[140] train acc: 100.00, validation acc: 89.10\n","[140] train acc: 100.00, validation acc: 89.00\n","[140] train acc: 100.00, validation acc: 89.00\n","[140] train acc: 99.80, validation acc: 88.80\n","[140] train acc: 99.84, validation acc: 89.00\n","[140] train acc: 99.87, validation acc: 89.70\n","[140] train acc: 99.89, validation acc: 90.00\n","[140] train acc: 99.80, validation acc: 90.60\n","[150] train acc: 99.22, validation acc: 91.90\n","[150] train acc: 99.61, validation acc: 92.30\n","[150] train acc: 99.74, validation acc: 92.30\n","[150] train acc: 99.80, validation acc: 92.80 - Saved the best model\n","[150] train acc: 99.84, validation acc: 92.80 - Saved the best model\n","[150] train acc: 99.61, validation acc: 92.80 - Saved the best model\n","[150] train acc: 99.67, validation acc: 92.40\n","[150] train acc: 99.70, validation acc: 92.60\n","[156] train acc: 100.00, validation acc: 92.90 - Saved the best model\n","[156] train acc: 99.78, validation acc: 93.00 - Saved the best model\n","[156] train acc: 99.80, validation acc: 93.10 - Saved the best model\n","[157] train acc: 100.00, validation acc: 93.20 - Saved the best model\n","[157] train acc: 100.00, validation acc: 93.20 - Saved the best model\n","[160] train acc: 100.00, validation acc: 91.50\n","[160] train acc: 100.00, validation acc: 91.30\n","[160] train acc: 100.00, validation acc: 91.10\n","[160] train acc: 100.00, validation acc: 90.90\n","[160] train acc: 100.00, validation acc: 90.70\n","[160] train acc: 100.00, validation acc: 90.40\n","[160] train acc: 100.00, validation acc: 90.60\n","[160] train acc: 100.00, validation acc: 90.30\n","[170] train acc: 100.00, validation acc: 91.00\n","[170] train acc: 99.61, validation acc: 90.80\n","[170] train acc: 99.74, validation acc: 90.70\n","[170] train acc: 99.61, validation acc: 90.90\n","[170] train acc: 99.69, validation acc: 90.80\n","[170] train acc: 99.61, validation acc: 90.60\n","[170] train acc: 99.55, validation acc: 90.30\n","[170] train acc: 99.50, validation acc: 90.60\n","[179] train acc: 99.74, validation acc: 93.40 - Saved the best model\n","[180] train acc: 99.22, validation acc: 92.40\n","[180] train acc: 99.61, validation acc: 91.90\n","[180] train acc: 99.74, validation acc: 91.70\n","[180] train acc: 99.61, validation acc: 91.60\n","[180] train acc: 99.69, validation acc: 91.00\n","[180] train acc: 99.74, validation acc: 90.60\n","[180] train acc: 99.78, validation acc: 89.80\n","[180] train acc: 99.80, validation acc: 90.00\n","[190] train acc: 99.22, validation acc: 89.10\n","[190] train acc: 99.22, validation acc: 87.90\n","[190] train acc: 99.48, validation acc: 88.00\n","[190] train acc: 99.41, validation acc: 87.40\n","[190] train acc: 99.38, validation acc: 87.90\n","[190] train acc: 99.48, validation acc: 88.10\n","[190] train acc: 99.44, validation acc: 87.50\n","[190] train acc: 99.50, validation acc: 87.40\n","[200] train acc: 100.00, validation acc: 89.30\n","[200] train acc: 100.00, validation acc: 89.20\n","[200] train acc: 100.00, validation acc: 89.10\n","[200] train acc: 100.00, validation acc: 88.90\n","[200] train acc: 99.84, validation acc: 89.10\n","[200] train acc: 99.74, validation acc: 89.10\n","[200] train acc: 99.67, validation acc: 89.30\n","[200] train acc: 99.50, validation acc: 88.20\n","[210] train acc: 100.00, validation acc: 87.20\n","[210] train acc: 100.00, validation acc: 86.90\n","[210] train acc: 99.74, validation acc: 86.70\n","[210] train acc: 99.80, validation acc: 86.50\n","[210] train acc: 99.84, validation acc: 86.40\n","[210] train acc: 99.87, validation acc: 86.20\n","[210] train acc: 99.89, validation acc: 86.00\n","[210] train acc: 99.70, validation acc: 86.10\n","[220] train acc: 100.00, validation acc: 88.10\n","[220] train acc: 99.22, validation acc: 88.40\n","[220] train acc: 99.48, validation acc: 88.90\n","[220] train acc: 99.61, validation acc: 89.50\n","[220] train acc: 99.69, validation acc: 89.80\n","[220] train acc: 99.74, validation acc: 90.20\n","[220] train acc: 99.78, validation acc: 90.00\n","[220] train acc: 99.70, validation acc: 89.60\n","[230] train acc: 100.00, validation acc: 87.60\n","[230] train acc: 100.00, validation acc: 87.80\n","[230] train acc: 100.00, validation acc: 87.70\n","[230] train acc: 100.00, validation acc: 87.50\n","[230] train acc: 100.00, validation acc: 87.50\n","[230] train acc: 100.00, validation acc: 87.20\n","[230] train acc: 99.89, validation acc: 87.20\n","[230] train acc: 99.80, validation acc: 88.10\n","[240] train acc: 100.00, validation acc: 86.00\n","[240] train acc: 100.00, validation acc: 85.80\n","[240] train acc: 100.00, validation acc: 85.40\n","[240] train acc: 100.00, validation acc: 85.30\n","[240] train acc: 100.00, validation acc: 85.50\n","[240] train acc: 100.00, validation acc: 85.80\n","[240] train acc: 100.00, validation acc: 85.80\n","[240] train acc: 99.90, validation acc: 85.80\n","[250] train acc: 100.00, validation acc: 91.30\n","[250] train acc: 100.00, validation acc: 91.30\n","[250] train acc: 99.74, validation acc: 90.90\n","[250] train acc: 99.41, validation acc: 90.70\n","[250] train acc: 99.53, validation acc: 89.50\n","[250] train acc: 99.61, validation acc: 87.70\n","[250] train acc: 99.67, validation acc: 86.60\n","[250] train acc: 99.70, validation acc: 85.40\n","[260] train acc: 100.00, validation acc: 85.50\n","[260] train acc: 100.00, validation acc: 85.90\n","[260] train acc: 100.00, validation acc: 86.50\n","[260] train acc: 99.80, validation acc: 87.40\n","[260] train acc: 99.84, validation acc: 88.00\n","[260] train acc: 99.87, validation acc: 88.60\n","[260] train acc: 99.89, validation acc: 88.90\n","[260] train acc: 99.90, validation acc: 89.10\n","[270] train acc: 100.00, validation acc: 88.00\n","[270] train acc: 100.00, validation acc: 88.40\n","[270] train acc: 100.00, validation acc: 88.40\n","[270] train acc: 100.00, validation acc: 88.40\n","[270] train acc: 100.00, validation acc: 88.40\n","[270] train acc: 100.00, validation acc: 88.50\n","[270] train acc: 100.00, validation acc: 88.50\n","[270] train acc: 100.00, validation acc: 88.70\n","[280] train acc: 100.00, validation acc: 88.70\n","[280] train acc: 100.00, validation acc: 88.50\n","[280] train acc: 100.00, validation acc: 88.40\n","[280] train acc: 99.80, validation acc: 89.30\n","[280] train acc: 99.84, validation acc: 89.80\n","[280] train acc: 99.87, validation acc: 90.70\n","[280] train acc: 99.89, validation acc: 90.70\n","[280] train acc: 99.90, validation acc: 91.20\n","[290] train acc: 100.00, validation acc: 86.80\n","[290] train acc: 100.00, validation acc: 87.30\n","[290] train acc: 100.00, validation acc: 87.90\n","[290] train acc: 100.00, validation acc: 88.20\n","[290] train acc: 100.00, validation acc: 88.90\n","[290] train acc: 100.00, validation acc: 89.00\n","[290] train acc: 100.00, validation acc: 89.20\n","[290] train acc: 100.00, validation acc: 89.90\n","[300] train acc: 100.00, validation acc: 88.80\n","[300] train acc: 99.61, validation acc: 88.10\n","[300] train acc: 99.48, validation acc: 87.10\n","[300] train acc: 99.61, validation acc: 85.50\n","[300] train acc: 99.53, validation acc: 84.70\n","[300] train acc: 99.48, validation acc: 84.60\n","[300] train acc: 99.33, validation acc: 86.00\n","[300] train acc: 99.40, validation acc: 86.20\n","[310] train acc: 100.00, validation acc: 88.70\n","[310] train acc: 100.00, validation acc: 89.00\n","[310] train acc: 100.00, validation acc: 89.60\n","[310] train acc: 100.00, validation acc: 89.20\n","[310] train acc: 100.00, validation acc: 89.10\n","[310] train acc: 99.87, validation acc: 88.80\n","[310] train acc: 99.89, validation acc: 88.20\n","[310] train acc: 99.90, validation acc: 87.70\n","[320] train acc: 100.00, validation acc: 90.60\n","[320] train acc: 100.00, validation acc: 90.70\n","[320] train acc: 100.00, validation acc: 90.30\n","[320] train acc: 100.00, validation acc: 90.40\n","[320] train acc: 99.84, validation acc: 90.40\n","[320] train acc: 99.87, validation acc: 90.60\n","[320] train acc: 99.89, validation acc: 90.70\n","[320] train acc: 99.90, validation acc: 90.60\n","[330] train acc: 99.22, validation acc: 92.00\n","[330] train acc: 99.61, validation acc: 92.10\n","[330] train acc: 99.74, validation acc: 92.00\n","[330] train acc: 99.80, validation acc: 92.10\n","[330] train acc: 99.84, validation acc: 91.80\n","[330] train acc: 99.74, validation acc: 91.70\n","[330] train acc: 99.67, validation acc: 91.40\n","[330] train acc: 99.70, validation acc: 90.80\n","[340] train acc: 100.00, validation acc: 87.40\n","[340] train acc: 99.61, validation acc: 87.00\n","[340] train acc: 99.74, validation acc: 87.20\n","[340] train acc: 99.80, validation acc: 87.40\n","[340] train acc: 99.84, validation acc: 86.70\n","[340] train acc: 99.87, validation acc: 86.20\n","[340] train acc: 99.78, validation acc: 86.30\n","[340] train acc: 99.60, validation acc: 87.10\n","[350] train acc: 100.00, validation acc: 86.10\n","[350] train acc: 98.83, validation acc: 87.20\n","[350] train acc: 99.22, validation acc: 88.70\n","[350] train acc: 99.02, validation acc: 89.60\n","[350] train acc: 99.22, validation acc: 90.30\n","[350] train acc: 99.35, validation acc: 90.30\n","[350] train acc: 99.44, validation acc: 90.10\n","[350] train acc: 99.50, validation acc: 90.30\n","[360] train acc: 100.00, validation acc: 90.00\n","[360] train acc: 100.00, validation acc: 89.90\n","[360] train acc: 100.00, validation acc: 89.80\n","[360] train acc: 100.00, validation acc: 89.90\n","[360] train acc: 100.00, validation acc: 89.70\n","[360] train acc: 99.87, validation acc: 89.60\n","[360] train acc: 99.78, validation acc: 90.00\n","[360] train acc: 99.80, validation acc: 89.70\n","[370] train acc: 100.00, validation acc: 88.70\n","[370] train acc: 100.00, validation acc: 88.40\n","[370] train acc: 100.00, validation acc: 88.30\n","[370] train acc: 100.00, validation acc: 87.80\n","[370] train acc: 100.00, validation acc: 87.60\n","[370] train acc: 100.00, validation acc: 87.50\n","[370] train acc: 100.00, validation acc: 87.60\n","[370] train acc: 100.00, validation acc: 87.70\n","[380] train acc: 100.00, validation acc: 90.20\n","[380] train acc: 100.00, validation acc: 90.50\n","[380] train acc: 99.74, validation acc: 90.40\n","[380] train acc: 99.80, validation acc: 90.10\n","[380] train acc: 99.84, validation acc: 89.40\n","[380] train acc: 99.74, validation acc: 88.90\n","[380] train acc: 99.78, validation acc: 88.30\n","[380] train acc: 99.80, validation acc: 88.20\n","[390] train acc: 100.00, validation acc: 87.30\n","[390] train acc: 100.00, validation acc: 87.30\n","[390] train acc: 100.00, validation acc: 87.30\n","[390] train acc: 100.00, validation acc: 87.30\n","[390] train acc: 100.00, validation acc: 87.20\n","[390] train acc: 100.00, validation acc: 87.20\n","[390] train acc: 100.00, validation acc: 87.00\n","[390] train acc: 100.00, validation acc: 87.10\n","[400] train acc: 100.00, validation acc: 88.50\n","[400] train acc: 100.00, validation acc: 88.40\n","[400] train acc: 100.00, validation acc: 88.40\n","[400] train acc: 100.00, validation acc: 88.30\n","[400] train acc: 100.00, validation acc: 88.30\n","[400] train acc: 100.00, validation acc: 88.30\n","[400] train acc: 100.00, validation acc: 88.30\n","[400] train acc: 100.00, validation acc: 88.20\n","[410] train acc: 99.22, validation acc: 82.60\n","[410] train acc: 99.61, validation acc: 82.40\n","[410] train acc: 99.48, validation acc: 82.50\n","[410] train acc: 99.41, validation acc: 82.70\n","[410] train acc: 99.53, validation acc: 82.70\n","[410] train acc: 99.61, validation acc: 83.00\n","[410] train acc: 99.67, validation acc: 83.10\n","[410] train acc: 99.60, validation acc: 83.20\n","[420] train acc: 100.00, validation acc: 87.70\n","[420] train acc: 100.00, validation acc: 87.50\n","[420] train acc: 100.00, validation acc: 87.50\n","[420] train acc: 100.00, validation acc: 87.50\n","[420] train acc: 100.00, validation acc: 87.50\n","[420] train acc: 100.00, validation acc: 88.20\n","[420] train acc: 100.00, validation acc: 88.70\n","[420] train acc: 99.90, validation acc: 88.90\n","[430] train acc: 100.00, validation acc: 89.00\n","[430] train acc: 100.00, validation acc: 89.10\n","[430] train acc: 100.00, validation acc: 89.20\n","[430] train acc: 100.00, validation acc: 89.30\n","[430] train acc: 100.00, validation acc: 89.40\n","[430] train acc: 100.00, validation acc: 89.40\n","[430] train acc: 100.00, validation acc: 89.30\n","[430] train acc: 100.00, validation acc: 89.20\n","[440] train acc: 99.22, validation acc: 90.40\n","[440] train acc: 99.61, validation acc: 90.50\n","[440] train acc: 99.74, validation acc: 90.50\n","[440] train acc: 99.80, validation acc: 90.50\n","[440] train acc: 99.84, validation acc: 90.40\n","[440] train acc: 99.87, validation acc: 90.40\n","[440] train acc: 99.89, validation acc: 90.40\n","[440] train acc: 99.90, validation acc: 90.40\n","[450] train acc: 100.00, validation acc: 90.40\n","[450] train acc: 100.00, validation acc: 90.40\n","[450] train acc: 100.00, validation acc: 90.30\n","[450] train acc: 100.00, validation acc: 90.30\n","[450] train acc: 99.84, validation acc: 90.20\n","[450] train acc: 99.87, validation acc: 90.00\n","[450] train acc: 99.89, validation acc: 89.60\n","[450] train acc: 99.90, validation acc: 89.30\n","[460] train acc: 100.00, validation acc: 90.40\n","[460] train acc: 100.00, validation acc: 90.50\n","[460] train acc: 100.00, validation acc: 90.50\n","[460] train acc: 100.00, validation acc: 90.60\n","[460] train acc: 100.00, validation acc: 90.90\n","[460] train acc: 100.00, validation acc: 90.70\n","[460] train acc: 100.00, validation acc: 91.00\n","[460] train acc: 100.00, validation acc: 90.90\n","[470] train acc: 99.22, validation acc: 90.70\n","[470] train acc: 99.61, validation acc: 90.60\n","[470] train acc: 99.74, validation acc: 90.20\n","[470] train acc: 99.80, validation acc: 89.90\n","[470] train acc: 99.84, validation acc: 89.70\n","[470] train acc: 99.87, validation acc: 89.70\n","[470] train acc: 99.89, validation acc: 89.50\n","[470] train acc: 99.90, validation acc: 89.40\n","[480] train acc: 100.00, validation acc: 91.00\n","[480] train acc: 100.00, validation acc: 90.90\n","[480] train acc: 100.00, validation acc: 91.00\n","[480] train acc: 100.00, validation acc: 90.80\n","[480] train acc: 100.00, validation acc: 90.70\n","[480] train acc: 99.87, validation acc: 90.30\n","[480] train acc: 99.89, validation acc: 90.00\n","[480] train acc: 99.90, validation acc: 89.80\n","[490] train acc: 100.00, validation acc: 91.60\n","[490] train acc: 99.61, validation acc: 91.40\n","[490] train acc: 99.74, validation acc: 90.90\n","[490] train acc: 99.80, validation acc: 90.40\n","[490] train acc: 99.84, validation acc: 90.20\n","[490] train acc: 99.74, validation acc: 90.50\n","[490] train acc: 99.78, validation acc: 91.10\n","[490] train acc: 99.70, validation acc: 91.00\n","[500] train acc: 100.00, validation acc: 82.20\n","[500] train acc: 99.61, validation acc: 82.00\n","[500] train acc: 99.74, validation acc: 83.10\n","[500] train acc: 99.41, validation acc: 84.30\n","[500] train acc: 99.53, validation acc: 84.90\n","[500] train acc: 99.48, validation acc: 85.50\n","[500] train acc: 99.44, validation acc: 86.00\n","[500] train acc: 99.50, validation acc: 86.70\n"]}]},{"cell_type":"code","source":["# 지도 학습 성능 평가하기 (베스트 모델)\n","\n","model.load_state_dict(torch.load('./models/cifar_model_for_pseudo_baseline.pth'))\n","accuracy(testloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEMAArB1ujoD","executionInfo":{"status":"ok","timestamp":1706793154395,"user_tz":-540,"elapsed":2462,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"c9adc0c0-3275-4ee6-a063-989e8295c9db"},"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["95.3"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["# 준지도 학습1을 위한 모델을 재정의하기\n","# 모델 파라미터를 초기화\n","\n","model = Net().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"9ACdWAmkz6gH","executionInfo":{"status":"ok","timestamp":1706793433835,"user_tz":-540,"elapsed":784,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["준지도 학습 1 수행하기\n","- 의사 라벨링은 정확한 라벨과 부정확한 라벨이 섞여 있으므로 훈련 데이터와 동일하게 모델 최적화에 사용한다면 좋지 못한 결과 초래\n","- 따라서 훈련 라벨과 의사 라벨을 구분하여 따로 손실 함수(L_t와 L_p)를 계산한 뒤 둘을 더해 손실 함수 L 정의\n","- 의사 라벨을 이용하는 손실 함수 부분에 가중치 alpha로 학습 개입 조절 가능\n","  - alpha=0이면 학습 데이터로만 모델 최적화\n","  - alpha가 커질수록 의사 라벨의 영향도가 더 커진다는 의미\n","  - 적절한 alpha 구하자"],"metadata":{"id":"Kcjgul4f1la1"}},{"cell_type":"code","source":["alpha = 0\n","alpha_t = 1e-4\n","# 처음 epoch 100번까지는 alpha=0 으로 학습 진행. 이후는 일정하게 alpha를 높여 진행\n","# 450회가 지나면 alpha를 alpha_t로 고정하여 학습 진행하여 마무리\n","T1 = 100\n","T2 = 450\n","best_acc = 0"],"metadata":{"id":"rPPLOrh70P7m","executionInfo":{"status":"ok","timestamp":1706794011562,"user_tz":-540,"elapsed":682,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["for epoch in range(501):\n","  correct = 0\n","  total = 0\n","  # zip을 이용하여 두 개의 데이터로더를 동시에 사용 가능함\n","  for traindata, pseudodata in zip(trainloader, unlabeledloader):\n","    inputs, labels = traindata[0].to(device), traindata[1].to(device)\n","    pinputs = pseudodata[0].float().to(device)\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    # 0보다 크면 의사 라벨로 함께 이용하여 손실 함수 계산\n","    if alpha > 0:\n","      poutputs = model(pinputs)\n","      _,plabels = torch.max(poutputs.detach(),1)\n","      loss = criterion(outputs,labels) + alpha*criterion(poutputs, plabels)\n","    # alpha=0 이면 학습 데이터만 이용하여 손실 함수 계산\n","    else:\n","      loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step() # 최적화\n","    # 학습 정확도 계산을 위해 정답 개수와 총 라벨 개수 누적\n","    _,predicted = torch.max(outputs.detach(),1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","  if (epoch > T1) and (epoch < T2):\n","    alpha = alpha_t*(epoch - T1)/(T2 - T1)\n","  elif epoch >= T2:\n","    alpha = alpha_t\n","  val_acc = accuracy(valloader)\n","  if val_acc >= best_acc:\n","    best_acc = val_acc\n","    torch.save(model.state_dict(), './models/cifar_model_for_pseudo_label.pth')\n","    print('[%d] train acc: %.2f, validation acc: %.2f - Saved the best model' %(epoch,100*correct/total,val_acc))\n","  elif epoch % 10 == 0:\n","    print('[%d] train acc: %.2f, validation acc: %.2f' %(epoch,100*correct/total,val_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4P7o4_p62cvA","executionInfo":{"status":"ok","timestamp":1706795197019,"user_tz":-540,"elapsed":223426,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"525fd81b-75bb-44df-9bea-32f38a7a6059"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["[0] train acc: 99.60, validation acc: 90.20\n","[5] train acc: 99.50, validation acc: 94.20 - Saved the best model\n","[10] train acc: 99.60, validation acc: 90.70\n","[20] train acc: 99.40, validation acc: 90.20\n","[30] train acc: 99.10, validation acc: 91.50\n","[40] train acc: 99.90, validation acc: 91.10\n","[50] train acc: 99.80, validation acc: 92.00\n","[60] train acc: 99.60, validation acc: 91.40\n","[70] train acc: 99.90, validation acc: 92.20\n","[80] train acc: 99.20, validation acc: 91.70\n","[90] train acc: 99.90, validation acc: 92.60\n","[100] train acc: 99.80, validation acc: 87.80\n","[110] train acc: 99.80, validation acc: 89.50\n","[120] train acc: 99.50, validation acc: 91.70\n","[130] train acc: 99.70, validation acc: 91.50\n","[140] train acc: 100.00, validation acc: 88.10\n","[150] train acc: 99.90, validation acc: 90.20\n","[160] train acc: 100.00, validation acc: 91.80\n","[170] train acc: 99.70, validation acc: 88.70\n","[180] train acc: 99.60, validation acc: 90.10\n","[190] train acc: 99.70, validation acc: 89.50\n","[200] train acc: 99.80, validation acc: 89.80\n","[210] train acc: 99.90, validation acc: 89.70\n","[220] train acc: 100.00, validation acc: 90.20\n","[230] train acc: 100.00, validation acc: 89.80\n","[240] train acc: 99.90, validation acc: 91.00\n","[250] train acc: 99.90, validation acc: 91.50\n","[260] train acc: 99.90, validation acc: 92.40\n","[270] train acc: 99.70, validation acc: 89.40\n","[280] train acc: 99.90, validation acc: 90.60\n","[290] train acc: 100.00, validation acc: 88.70\n","[300] train acc: 99.90, validation acc: 91.40\n","[310] train acc: 99.90, validation acc: 90.30\n","[320] train acc: 99.90, validation acc: 87.50\n","[330] train acc: 99.90, validation acc: 89.40\n","[340] train acc: 99.90, validation acc: 89.30\n","[350] train acc: 99.70, validation acc: 89.40\n","[360] train acc: 100.00, validation acc: 88.30\n","[370] train acc: 100.00, validation acc: 88.20\n","[380] train acc: 100.00, validation acc: 90.40\n","[390] train acc: 100.00, validation acc: 89.50\n","[400] train acc: 100.00, validation acc: 89.80\n","[410] train acc: 99.90, validation acc: 89.10\n","[420] train acc: 100.00, validation acc: 90.70\n","[430] train acc: 99.90, validation acc: 88.30\n","[440] train acc: 99.80, validation acc: 86.90\n","[450] train acc: 99.50, validation acc: 86.40\n","[460] train acc: 99.40, validation acc: 90.80\n","[470] train acc: 100.00, validation acc: 89.50\n","[480] train acc: 100.00, validation acc: 90.20\n","[490] train acc: 99.80, validation acc: 91.50\n","[500] train acc: 99.80, validation acc: 90.20\n"]}]},{"cell_type":"code","source":["# 준지도 학습1 성능 평가하기\n","# 평가 정확도 내려감...?\n","\n","model.load_state_dict(torch.load('./models/cifar_model_for_pseudo_label.pth'))\n","accuracy(testloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_Df_gDS7CJH","executionInfo":{"status":"ok","timestamp":1706795285169,"user_tz":-540,"elapsed":2141,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"af228df5-ef46-49db-b2bf-024da86e6d2e"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["95.14"]},"metadata":{},"execution_count":89}]},{"cell_type":"markdown","source":["준지도 학습 2 를 위한 모델을 재정의하기"],"metadata":{"id":"K-mkV5Dr7dNI"}},{"cell_type":"code","source":["# 이번 방법에서는 학습 데이터로 학습된 사전 훈련 모델을 가지고 의사 라벨을 생성한 뒤 이를 실제 라벨처럼 사용해 봄\n","\n","model = Net().to(device)\n","model.load_state_dict(torch.load('./models/cifar_model_for_pseudo_baseline.pth'))\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"metadata":{"id":"F7Jj-LmV7gUd","executionInfo":{"status":"ok","timestamp":1706795475507,"user_tz":-540,"elapsed":466,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# 의사 라벨 생성하기\n","\n","# 사전 학습 모델로 예측된 출력 벡터의 가장 큰 원소가 0.99가 넘으면 의사 라벨로 사용\n","pseudo_threshold = 0.99\n","# 데이터를 모을 빈 텐서\n","pseudo_images = torch.tensor([],dtype=torch.float)\n","pseudo_labels = torch.tensor([],dtype=torch.long)\n","with torch.no_grad():\n","  for data in tqdm(unlabeledloader):\n","    model.eval()\n","    images = data[0].float().to(device)\n","    outputs = model(images)\n","    # 출력 벡터의 원소는 0과 1 사이의 값이 아닐 수 있음. 따라서 값은 0부터 1까지로 만들어 주기 위해 softmax 함수 사용\n","    outputs = torch.nn.functional.softmax(outputs,dim=1)\n","    max_val, predicted = torch.max(outputs.detach(),1)\n","    # 만약 현재 배치에 0.99 이상을 만족하는 데이터가 1개 이상 있다면\n","    idx = np.where(max_val.cpu().numpy() >= pseudo_threshold)[0]\n","    if len(idx) > 0:\n","      # 데이터 누적\n","      pseudo_images = torch.cat((pseudo_images, images.cpu()[idx]),0)\n","      pseudo_labels = torch.cat((pseudo_labels, predicted.cpu()[idx]),0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3dV9Eg98COv","executionInfo":{"status":"ok","timestamp":1706796715376,"user_tz":-540,"elapsed":18840,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"c26935d3-c6d7-436a-dcda-18e99a72a219"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 227/227 [00:18<00:00, 12.32it/s]\n"]}]},{"cell_type":"code","source":["# 준지도 학습2를 위한 데이터로더 정의하기\n","\n","pseudo_dataset = MyDataset(pseudo_images, pseudo_labels)\n","pseudoloader = torch.utils.data.DataLoader(pseudo_dataset,batch_size=256,shuffle=True)"],"metadata":{"id":"iXdvTzggAxfM","executionInfo":{"status":"ok","timestamp":1706796780194,"user_tz":-540,"elapsed":593,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["# 준지도 학습2 수행하기\n","\n","alpha = 0\n","alpha_t = 1e-4\n","# 처음 epoch 20번까지는 alpha=0 으로 학습 진행. 이후는 일정하게 alpha를 높여 진행\n","# 450회가 지나면 alpha를 alpha_t로 고정하여 학습 진행하여 마무리\n","T1 = 20\n","T2 = 450\n","best_acc = 0"],"metadata":{"id":"irNyHZTsBA0P","executionInfo":{"status":"ok","timestamp":1706796826783,"user_tz":-540,"elapsed":673,"user":{"displayName":"문소연","userId":"00452830055130923194"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["for epoch in range(501):\n","  correct = 0\n","  total = 0\n","  # zip을 이용하여 두 개의 데이터로더를 동시에 사용 가능함\n","  for traindata, pseudodata in zip(trainloader, pseudoloader):\n","    inputs, labels = traindata[0].to(device), traindata[1].to(device) # 학습 데이터 불러옴\n","    pinputs, plabels = pseudodata[0].to(device), pseudodata[1].to(device) # 의사 라벨 데이터 불러옴\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    poutputs = model(pinputs)\n","    loss = criterion(outputs,labels) + alpha*criterion(poutputs, plabels)\n","    loss.backward()\n","    optimizer.step() # 최적화\n","    # 학습 정확도 계산을 위해 정답 개수와 총 라벨 개수 누적\n","    _,predicted = torch.max(outputs.detach(),1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","  if (epoch > T1) and (epoch < T2):\n","    alpha = alpha_t*(epoch - T1)/(T2 - T1)\n","  elif epoch >= T2:\n","    alpha = alpha_t\n","  val_acc = accuracy(valloader)\n","  if val_acc >= best_acc:\n","    best_acc = val_acc\n","    torch.save(model.state_dict(), './models/cifar_model_for_pseudo_label2.pth')\n","    print('[%d] train acc: %.2f, validation acc: %.2f - Saved the best model' %(epoch,100*correct/total,val_acc))\n","  elif epoch % 10 == 0:\n","    print('[%d] train acc: %.2f, validation acc: %.2f' %(epoch,100*correct/total,val_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtc2QWbDBMHp","executionInfo":{"status":"ok","timestamp":1706797387412,"user_tz":-540,"elapsed":225659,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"8014baef-bb7c-4ae5-f06e-85060f183a49"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["[0] train acc: 100.00, validation acc: 92.00 - Saved the best model\n","[5] train acc: 99.20, validation acc: 92.30 - Saved the best model\n","[8] train acc: 99.60, validation acc: 92.30 - Saved the best model\n","[9] train acc: 99.30, validation acc: 93.10 - Saved the best model\n","[10] train acc: 99.70, validation acc: 89.80\n","[20] train acc: 99.80, validation acc: 89.20\n","[30] train acc: 99.80, validation acc: 92.40\n","[33] train acc: 99.60, validation acc: 94.10 - Saved the best model\n","[40] train acc: 99.80, validation acc: 93.60\n","[46] train acc: 99.70, validation acc: 94.50 - Saved the best model\n","[50] train acc: 99.70, validation acc: 92.90\n","[60] train acc: 99.70, validation acc: 92.30\n","[70] train acc: 99.70, validation acc: 93.30\n","[80] train acc: 100.00, validation acc: 93.50\n","[90] train acc: 99.80, validation acc: 93.00\n","[100] train acc: 99.80, validation acc: 92.90\n","[110] train acc: 99.90, validation acc: 93.10\n","[120] train acc: 99.80, validation acc: 92.60\n","[130] train acc: 99.90, validation acc: 91.40\n","[140] train acc: 99.40, validation acc: 91.20\n","[150] train acc: 100.00, validation acc: 91.90\n","[160] train acc: 100.00, validation acc: 92.20\n","[170] train acc: 99.90, validation acc: 88.80\n","[180] train acc: 99.90, validation acc: 92.10\n","[190] train acc: 99.90, validation acc: 84.90\n","[200] train acc: 99.80, validation acc: 81.40\n","[210] train acc: 99.90, validation acc: 84.20\n","[220] train acc: 99.70, validation acc: 82.70\n","[230] train acc: 99.90, validation acc: 69.80\n","[240] train acc: 99.80, validation acc: 77.40\n","[250] train acc: 99.80, validation acc: 74.30\n","[260] train acc: 100.00, validation acc: 62.90\n","[270] train acc: 100.00, validation acc: 70.80\n","[280] train acc: 99.90, validation acc: 68.60\n","[290] train acc: 100.00, validation acc: 74.40\n","[300] train acc: 99.90, validation acc: 69.40\n","[310] train acc: 99.60, validation acc: 71.30\n","[320] train acc: 99.70, validation acc: 71.20\n","[330] train acc: 99.90, validation acc: 49.30\n","[340] train acc: 99.80, validation acc: 52.30\n","[350] train acc: 100.00, validation acc: 59.50\n","[360] train acc: 99.90, validation acc: 52.10\n","[370] train acc: 100.00, validation acc: 56.10\n","[380] train acc: 99.90, validation acc: 57.20\n","[390] train acc: 99.90, validation acc: 61.70\n","[400] train acc: 99.90, validation acc: 55.10\n","[410] train acc: 100.00, validation acc: 58.70\n","[420] train acc: 100.00, validation acc: 60.20\n","[430] train acc: 100.00, validation acc: 55.70\n","[440] train acc: 100.00, validation acc: 60.30\n","[450] train acc: 100.00, validation acc: 65.50\n","[460] train acc: 100.00, validation acc: 59.40\n","[470] train acc: 100.00, validation acc: 57.80\n","[480] train acc: 100.00, validation acc: 59.30\n","[490] train acc: 100.00, validation acc: 57.70\n","[500] train acc: 99.50, validation acc: 56.60\n"]}]},{"cell_type":"code","source":["# 준지도 학습2 성능 평가하기\n","\n","model.load_state_dict(torch.load('./models/cifar_model_for_pseudo_label2.pth'))\n","accuracy(testloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XC8qS0zKDbHS","executionInfo":{"status":"ok","timestamp":1706797468468,"user_tz":-540,"elapsed":1989,"user":{"displayName":"문소연","userId":"00452830055130923194"}},"outputId":"9df413b6-693c-437e-cd07-5774f8bda4bc"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["95.43"]},"metadata":{},"execution_count":96}]}]}